{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cc87bac-8e47-4d62-9d19-851d8510c071",
   "metadata": {},
   "source": [
    "# 00 LSTM trefor park"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a53ff3e-2f69-4006-b19c-664cfd80250e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from core.util.plot_predictions import plot_predictions\n",
    "from core.util.save_model import save_model, load_parameters\n",
    "from core.util.get_datasets import cross_validation\n",
    "from core.util.trefor_dataset import TreforData\n",
    "from core.models import LSTM\n",
    "from core.models.model_training import train_model, test_model\n",
    "from core.util.hyperparameter_configuration import get_hyperparameter_configuration\n",
    "from core.util.metrics import mae, rmse, smape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8a7b4c-5116-49ba-8b00-8cff736e5db6",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Parameters specific to this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47828627-3897-4244-8732-828b5cfdf3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"00_LSTM_trefor_park\"\n",
    "features = {}\n",
    "model_input_size = len(features) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38beebe-384a-4011-858b-04e300cbfcd5",
   "metadata": {},
   "source": [
    "Load in the 3 best hyperparameter configurations found by the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2709134-13ba-4756-a1ce-e40dbe6b307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = []\n",
    "for i in range(3):\n",
    "    parameters.append(load_parameters(f\"{experiment_name}_{i}\"))\n",
    "    print(parameters[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5711f1ba-3a7d-4592-8018-447c5fc87d94",
   "metadata": {},
   "source": [
    "Global hyperparameter configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a683a8f-f885-4436-ada6-2c8102eab202",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = get_hyperparameter_configuration()\n",
    "hidden_size = hyperparameters[\"hidden_size\"]\n",
    "epochs = hyperparameters[\"epochs\"]\n",
    "horizon = hyperparameters[\"horizon\"]\n",
    "loss_function = hyperparameters[\"loss_function\"]\n",
    "dropout_rate = hyperparameters[\"dropout_rate\"]\n",
    "train_days = hyperparameters[\"train_days\"]\n",
    "val_days = hyperparameters[\"val_days\"]\n",
    "test_days = hyperparameters[\"test_days\"]\n",
    "early_stopper = hyperparameters[\"early_stopper\"]\n",
    "\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc9a437-5750-4504-a953-fb658163a2b8",
   "metadata": {},
   "source": [
    "If the host has CUDA, it will use the GPU for computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a053ac44-d64a-4c3f-a03f-4a2ffd228494",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11056a11-a755-48ce-ade3-fe2ca4fae3ce",
   "metadata": {},
   "source": [
    "### Creation of our simple LSTM model\n",
    "The implementation consist of three layers, defined in the `forward` method.\n",
    "1. LSTM\n",
    "2. LeakyReLU\n",
    "3. Linear\n",
    "4. ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1a163b-50cb-4c18-b85a-009fe7c7c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_layers: int, lookback: int) -> nn.Module:\n",
    "    \"\"\"Get the model for training folds.\"\"\"\n",
    "    model = LSTM(\n",
    "        input_size=model_input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        dropout_rate=dropout_rate,\n",
    "        horizon=horizon,\n",
    "        lookback=lookback,\n",
    "    )\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855b7097-b2c0-4704-a557-e556d23e43e3",
   "metadata": {},
   "source": [
    "### Main loop\n",
    "Iterate all hyperparameter configuration to find the best one.\n",
    "\n",
    "For each of these, we do the full iteration of epochs (unless early stop occurs) with training and validation.\n",
    "Lastly, we run the test set on the given model to see how it performs using the metrics MAE, RMSE, and sMAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc0224-38da-4f18-ad98-b00c414c5168",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(3):\n",
    "    # Reset the early stopper\n",
    "    # Otherwise it can carry information from the previous training and stops too early\n",
    "    early_stopper.reset()\n",
    "\n",
    "    # Get parameters for the i'th model\n",
    "    experiment_parameters = parameters[i]\n",
    "    learning_rate = experiment_parameters[\"learning_rate\"]\n",
    "    batch_size = experiment_parameters[\"batch_size\"]\n",
    "    lookback = experiment_parameters[\"lookback\"]\n",
    "    num_layers = experiment_parameters[\"num_layers\"]\n",
    "    torch.manual_seed(experiment_parameters[\"seed\"])\n",
    "\n",
    "    # Loads in the datasets because they can differ for the models (different lookback)\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test = cross_validation(\n",
    "        lookback=lookback,\n",
    "        horizon=horizon,\n",
    "        train_days=train_days,\n",
    "        val_days=val_days,\n",
    "        test_days=test_days,\n",
    "        features=features,\n",
    "    )\n",
    "\n",
    "    # Show the shapes of the datasets\n",
    "    # x_train, x_validation, and x_test: [datapoints, lookback, number of features]\n",
    "    # y_train, y_validation, and y_test: [datapoints, horizon]\n",
    "    print(f\"x_train: {x_train.shape}\")\n",
    "    print(f\"y_train: {y_train.shape}\")\n",
    "    print(f\"x_val: {x_val.shape}\")\n",
    "    print(f\"y_val: {y_val.shape}\")\n",
    "    print(f\"x_test: {x_test.shape}\")\n",
    "    print(f\"y_test: {y_test.shape}\")\n",
    "\n",
    "    # convert to dataset that can use dataloaders\n",
    "    train_dataset = TreforData(x_train, y_train, device)\n",
    "    val_dataset = TreforData(x_val, y_val, device)\n",
    "    test_dataset = TreforData(x_test, y_test, device)\n",
    "\n",
    "    # initialize the dataloaders, without shuffeling the data between epochs\n",
    "    training_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    validation_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    testing_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Train the i'th model\n",
    "    experiment_iteration_name = f\"00_LSTM_trefor_park_iteration_{i}\"\n",
    "    best_train_loss, best_val_loss, best_model = train_model(\n",
    "        epochs=epochs,\n",
    "        model=get_model(num_layers, lookback),\n",
    "        loss_function=loss_function,\n",
    "        training_loader=training_loader,\n",
    "        validation_loader=validation_loader,\n",
    "        learning_rate=learning_rate,\n",
    "        early_stopper=early_stopper,\n",
    "    )\n",
    "\n",
    "    t_loss, predicted = test_model(\n",
    "        best_model=best_model,\n",
    "        loss_function=loss_function,\n",
    "        testing_loader=testing_loader,\n",
    "    )\n",
    "\n",
    "    # Flatten the predictions and test set\n",
    "    # This is done so they can be compared to the tests and calculate the metrics\n",
    "    flattened_predicted = predicted.flatten()\n",
    "    flattened_test = y_test.flatten()\n",
    "\n",
    "    model_mae = mae(flattened_test, flattened_predicted).detach().item()\n",
    "    model_rmse = rmse(flattened_test, flattened_predicted)\n",
    "    model_smape = smape(flattened_test, flattened_predicted).detach().item()\n",
    "\n",
    "    print(experiment_iteration_name)\n",
    "    print(f\"MAE: {model_mae}\")\n",
    "    print(f\"RMSE: {model_rmse}\")\n",
    "    print(f\"SMAPE: {model_smape} \\n\")\n",
    "\n",
    "    # Append the results so we can pick out the second best\n",
    "    results.append(\n",
    "        {\n",
    "            \"sMAPE\": model_smape,\n",
    "            \"MAE\": model_mae,\n",
    "            \"RMSE\": model_rmse,\n",
    "            \"model\": best_model,\n",
    "            \"train loss\": best_train_loss,\n",
    "            \"validation loss\": best_val_loss,\n",
    "            \"y_test\": y_test,\n",
    "            \"predicted\": predicted,\n",
    "            \"parameter index\": i,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82754718",
   "metadata": {},
   "source": [
    "In order to select the second best model we sort the results by lowest sMAPE.\n",
    "\n",
    "sMAPE is the chosen metric as the loss is lower when predictions are larger than the actual value compared to when predictions are lower than the actual value.\n",
    "$$\\hat{y} > y: \\text{lower loss} \\\\ \\hat{y} < y: \\text{greater loss}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043a7963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort results and extract second best\n",
    "second_best = sorted(results, key=lambda d: d[\"sMAPE\"])[1]\n",
    "print(f\"sMAPE: {second_best[\"sMAPE\"]}\")\n",
    "print(f\"MAE: {second_best[\"MAE\"]}\")\n",
    "print(f\"RMSE: {second_best[\"RMSE\"]}\")\n",
    "print(f\"parameters: {parameters[second_best[\"parameter index\"]]}\")\n",
    "\n",
    "model_smape = second_best[\"sMAPE\"]\n",
    "model_mae = second_best[\"MAE\"]\n",
    "model_rmse = second_best[\"RMSE\"]\n",
    "best_model = second_best[\"model\"]\n",
    "best_train_loss = second_best[\"train loss\"]\n",
    "best_val_loss = second_best[\"validation loss\"]\n",
    "y_test = second_best[\"y_test\"]\n",
    "predicted = second_best[\"predicted\"]\n",
    "\n",
    "save_model(\n",
    "    model=best_model,\n",
    "    model_name=experiment_name,\n",
    "    train_loss=best_train_loss,\n",
    "    val_loss=best_val_loss,\n",
    "    mae=model_mae,\n",
    "    rmse=model_rmse,\n",
    "    smape=model_smape,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4019724b-7b41-4422-8635-ff06c284904a",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "### Training- and validation loss\n",
    "Visualize the training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb981f6a-ca62-46ce-9a6f-53dc9c670cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(best_train_loss, label=\"Training Loss\")\n",
    "plt.plot(best_val_loss, label=\"Validation Loss\")\n",
    "plt.scatter(\n",
    "    best_val_loss.index(min(best_val_loss)),\n",
    "    min(best_val_loss),\n",
    "    color=\"red\",\n",
    "    marker=\".\",\n",
    "    label=\"Chosen model\",\n",
    "    zorder=10,\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b2bfa2-bc6f-45c3-ac26-51857da4f299",
   "metadata": {},
   "source": [
    "### Predictions- and actual consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267411df-812e-4049-9961-296de7f158b6",
   "metadata": {},
   "source": [
    "Plot the actual values and predictions from the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f854da9-7834-4241-a797-c0059bcf13d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(0, 300, y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45558be-e8f1-4e5d-9e1c-7c0e845d00ed",
   "metadata": {},
   "source": [
    "Plot predictions for the first 7 days in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ef7ae6-5533-43ee-b7d0-9ed2328bd044",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(0, 6, y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e5245a-4d7d-485c-ae4c-a5c9dc7f490e",
   "metadata": {},
   "source": [
    "Plot predictions for the last 7 days in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17c784d-833a-4a74-8443-0556b82c7086",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(-7, -1, y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d3761b-8ed9-4666-ab12-839242020634",
   "metadata": {},
   "source": [
    "Plot predictions for 7 days in the in the middle of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29fcd2e-fdcb-4a32-bc3e-1d1e21b33e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_count = len(y_test) / 24\n",
    "plot_predictions(int(days_count / 2), int(days_count / 2 + 7), y_test, predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
