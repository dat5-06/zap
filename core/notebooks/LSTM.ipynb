{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a53ff3e-2f69-4006-b19c-664cfd80250e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from core.util.get_datasets import get_timeseries_dataset\n",
    "from core.util.io import read_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8a7b4c-5116-49ba-8b00-8cff736e5db6",
   "metadata": {},
   "source": [
    "Set global parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcb569a4-9faf-462b-858e-b9f1a64e4a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "EPOCHS = 250\n",
    "lookback = 24\n",
    "loss_function = nn.HuberLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f8a0a7-1561-4ea6-bd3d-d9b5c6636e45",
   "metadata": {},
   "source": [
    "Read the preprocessed Trefor household data as a timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55d3ce1f-1ce1-4eaf-ac7b-8a0bbccfa482",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_csv(\"core/data/processed/park_training.csv\")\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc9a437-5750-4504-a953-fb658163a2b8",
   "metadata": {},
   "source": [
    "Use CUDA (GPU) if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a053ac44-d64a-4c3f-a03f-4a2ffd228494",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e599066f-16bb-480a-a99b-f3aae3e15847",
   "metadata": {},
   "source": [
    "Normalize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b38c776a-cf18-4df4-b05b-b4790435d2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "# data_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e021e31-6f02-4023-9ccb-486d46edce4a",
   "metadata": {},
   "source": [
    "Split data into a training, validation, and test set. Output of the get_timeserie_dataset are tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "414d37c9-84ef-4812-8665-e5de40b15c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, x_test, y_train, y_val, y_test = get_timeseries_dataset(\n",
    "    data_normalized, lookback\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb888824-b5ef-45d6-a5dd-dd679f79d913",
   "metadata": {},
   "source": [
    "Create the datasets for train, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "484026da-9e17-4d8f-a210-555b275b6574",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreforData(Dataset):\n",
    "    \"\"\"Initialize Trefor dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, x: torch.tensor, y: torch.tensor) -> None:\n",
    "        \"\"\"Initialize dataset.\n",
    "\n",
    "        Arguments:\n",
    "            x: feature as torch\n",
    "            y: target as torch\n",
    "\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return length of dataset.\"\"\"\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, i: int) -> tuple:\n",
    "        \"\"\"Return tuple from dataset.\"\"\"\n",
    "        return self.x[i], self.y[i]\n",
    "\n",
    "\n",
    "train_dataset = TreforData(x_train, y_train)\n",
    "val_dataset = TreforData(x_val, y_val)\n",
    "test_dataset = TreforData(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150b1c41-512a-4839-8bf4-d3d44b3f22ba",
   "metadata": {},
   "source": [
    "Load data into a dataloader with specified batch size from global parameter. Don't shuffle as we use time series were order matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fba07b47-1a17-4fd5-8cc2-9b7211c09097",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "validation_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "testing_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11056a11-a755-48ce-ade3-fe2ca4fae3ce",
   "metadata": {},
   "source": [
    "Initialize a very basic LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1a163b-50cb-4c18-b85a-009fe7c7c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\"Super scuffed LSTM.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        num_stacked_layers: int,\n",
    "        output_size: int,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the LSTM and its layers.\"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_stacked_layers = num_stacked_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_stacked_layers, batch_first=True\n",
    "        )\n",
    "\n",
    "        # self.fc1 = nn.Linear((input_size * hidden_size), hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"Define the forward pass.\"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(\n",
    "            device\n",
    "        )\n",
    "        c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(\n",
    "            device\n",
    "        )\n",
    "\n",
    "        x, _ = self.lstm(x, (h0, c0))\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x[:, -1, :])\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "model = LSTM(input_size=1, hidden_size=15, num_stacked_layers=5, output_size=24)\n",
    "model.to(device)\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afd81b2-033b-4e7d-b6fc-8a108bc315cc",
   "metadata": {},
   "source": [
    "Function for training one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1536bea4-ae30-4bae-abd4-09841edf2d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "plot_train_loss = []\n",
    "plot_val_loss = []\n",
    "\n",
    "\n",
    "def train_one_epoch() -> float:\n",
    "    \"\"\"Train one epoch.\"\"\"\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + target\n",
    "        inputs, target = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        predictions = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_function(predictions, target)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100  # loss per 100 batch\n",
    "            # print(f'  batch {i+1} loss: {last_loss}')\n",
    "            running_loss = 0.0\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855b7097-b2c0-4704-a557-e556d23e43e3",
   "metadata": {},
   "source": [
    "Validate the training model on the validation set for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc0224-38da-4f18-ad98-b00c414c5168",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_number = 0\n",
    "best_v_loss = sys.maxsize\n",
    "best_model = None\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"EPOCH {epoch_number + 1}:\")\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch()\n",
    "\n",
    "    running_v_loss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, v_data in enumerate(validation_loader):\n",
    "            v_inputs, v_target = v_data\n",
    "            v_predictions = model(v_inputs)\n",
    "            v_loss = loss_function(v_predictions, v_target)\n",
    "            running_v_loss += v_loss.item()\n",
    "\n",
    "    avg_v_loss = running_v_loss / (i + 1)\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    # print(f'Training : {avg_loss}, Validation : {avg_v_loss}')\n",
    "    plot_train_loss.append(avg_loss)\n",
    "    plot_val_loss.append(avg_v_loss)\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_v_loss < best_v_loss:\n",
    "        best_v_loss = avg_v_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4f71a8-6d6f-47eb-b3b3-ac74740eaf70",
   "metadata": {},
   "source": [
    "Evaluate the final model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a53c73-830d-46e5-a6e5-dd0aa36db9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "predicted = []\n",
    "t_loss = 0\n",
    "num_batches = len(testing_loader)\n",
    "size = len(testing_loader.dataset)\n",
    "with torch.no_grad():\n",
    "    for i, t_data in enumerate(testing_loader):\n",
    "        t_inputs, t_target = t_data\n",
    "        t_predictions = best_model(t_inputs)\n",
    "        predicted.append(t_predictions)\n",
    "        t_loss += loss_function(t_predictions, t_target).item()\n",
    "\n",
    "predicted = torch.cat(predicted, dim=0)\n",
    "t_loss /= num_batches\n",
    "print(f\"Avg loss: {t_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267411df-812e-4049-9961-296de7f158b6",
   "metadata": {},
   "source": [
    "Visualize the training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb981f6a-ca62-46ce-9a6f-53dc9c670cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(plot_train_loss, label=\"Training Loss\")\n",
    "plt.plot(plot_val_loss, label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4019724b-7b41-4422-8635-ff06c284904a",
   "metadata": {},
   "source": [
    "Visualize model performance. Actual consumption plotted with the predicted consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17c784d-833a-4a74-8443-0556b82c7086",
   "metadata": {},
   "outputs": [],
   "source": [
    "denormalized_y_test = scaler.inverse_transform(y_test.numpy())\n",
    "denormalized_predicted = scaler.inverse_transform(predicted.numpy())\n",
    "plt.plot(denormalized_y_test, label=\"Actual Consumption\")\n",
    "plt.plot(denormalized_predicted, label=\"Predicted Consumption\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Consumption\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
